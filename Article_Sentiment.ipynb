{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Code\n",
    "\n",
    "`pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "pip install spacy-lookups-data\n",
    "pip install label-studio`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating\n",
    "\n",
    "Add to the corpus from a news article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# URL of the article to scrape\n",
    "url = 'https://www.hrw.org/world-report/2020/country-chapters/israel-and-palestine'\n",
    "\n",
    "# Send a GET request to the URL and get the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract the text content from each <p> element and store in a JSON object\n",
    "data = {\"paragraphs\": []}\n",
    "for p in soup.find_all('p'):\n",
    "    text = p.get_text().strip()\n",
    "    if len(text) > 0:\n",
    "        data[\"paragraphs\"].append({\"text\": text})\n",
    "df = pd.DataFrame(data[\"paragraphs\"])\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('paragraphs.csv', index=False)\n",
    "# Save the DataFrame as a JSONL file\n",
    "with open('paragraphs.jsonl', 'w') as f:\n",
    "    for _, row in df.iterrows():\n",
    "        json.dump(row.to_dict(), f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to the corpus a set of manual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "# List of phrases for each label\n",
    "israel_phrases = [\n",
    "    'Israeli settlements in the West Bank',\n",
    "    'Jerusalem as the capital of Israel',\n",
    "    'Iron Dome and Israel’s missile defense',\n",
    "    'Likud and the Israeli right-wing',\n",
    "    'Arab-Israeli conflict and peace talks',\n",
    "    'Mossad and Israel’s intelligence agency',\n",
    "    'Jewish National Fund and land ownership',\n",
    "    'Knesset and Israeli politics',\n",
    "    'Yitzhak Rabin and the assassination',\n",
    "    'Israeli-Palestinian coexistence initiatives',\n",
    "    \"Jerusalem\",\n",
    "    \"West Bank\",\n",
    "    \"Gaza\",\n",
    "    \"Netanyahu\",\n",
    "    \"Israeli-Palestinian conflict\",\n",
    "    \"Jewish settlements\",\n",
    "    \"Iron Dome\",\n",
    "    \"Masada\",\n",
    "    \"Yom Kippur War\",\n",
    "    \"Six-Day War\"\n",
    "]\n",
    "\n",
    "palestine_phrases = [\n",
    "    'Palestinian refugees and global politics',\n",
    "    'Occupation of the West Bank and East Jerusalem',\n",
    "    'Gaza Strip and the Israeli blockade',\n",
    "    'Palestinian Authority and its governance',\n",
    "    'Hamas and its role in Palestinian politics',\n",
    "    'Al-Aqsa Mosque and Temple Mount',\n",
    "    'Palestinian Nationalism and the PLO',\n",
    "    'Oslo Accords and the peace process',\n",
    "    'BDS Movement and its impact',\n",
    "    'Intifada and resistance movements',\n",
    "    \"West Bank\",\n",
    "    \"Gaza\",\n",
    "    \"Hamas\",\n",
    "    \"Palestinian Authority\",\n",
    "    \"Israeli-Palestinian conflict\",\n",
    "    \"Al-Aqsa Mosque\",\n",
    "    \"Intifada\",\n",
    "    \"Right of Return\",\n",
    "    \"Two-State Solution\",\n",
    "    \"Nakba\"\n",
    "]\n",
    "\n",
    "judaism_phrases = [\n",
    "    'Jewish diaspora and global communities',\n",
    "    'Torah and Jewish law',\n",
    "    'Anti-Semitism and Jewish discrimination',\n",
    "    'Jewish settlements in the West Bank',\n",
    "    'Hasidic Judaism and its practices',\n",
    "    'Zionism and Jewish nationalism',\n",
    "    'Kabbalah and Jewish mysticism',\n",
    "    'Jewish festivals and holidays',\n",
    "    'Talmud and Jewish scholarship',\n",
    "    'Holocaust and Jewish history',\n",
    "    \"Torah\",\n",
    "    \"Talmud\",\n",
    "    \"Rabbi\",\n",
    "    \"Synagogue\",\n",
    "    \"Kabbalah\",\n",
    "    \"Hasidism\",\n",
    "    \"Passover\",\n",
    "    \"Yom Kippur\",\n",
    "    \"Hanukkah\",\n",
    "    \"Bar Mitzvah\"\n",
    "]\n",
    "\n",
    "# Generate list of labeled data\n",
    "labeled_data = []\n",
    "for israel_text in israel_phrases:\n",
    "    israel_label = [israel_text, [[0, len(israel_text), \"ISRAEL\"]]]\n",
    "    labeled_data.append(israel_label)\n",
    "\n",
    "for palestine_text in palestine_phrases:\n",
    "    palestine_label = [palestine_text, [[0, len(palestine_text), \"PALESTINE\"]]]\n",
    "    labeled_data.append(palestine_label)\n",
    "\n",
    "for judaism_text in judaism_phrases:\n",
    "    judaism_label = [judaism_text, [[0, len(judaism_text), \"JUDAISM\"]]]\n",
    "    labeled_data.append(judaism_label)\n",
    "\n",
    "# Write data to JSONL file\n",
    "with jsonlines.open('phrases.jsonl', mode='w') as writer:\n",
    "    for item in labeled_data:\n",
    "        data = {\n",
    "            'text': item[0],\n",
    "            'label': item[1]\n",
    "        }\n",
    "        writer.write(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating\n",
    "\n",
    "Open an instance of doccano to upload `paragraphs.jsonl` and `phrases.jsonl` and manually check or add labels.\n",
    "\n",
    "Save these labels then to a file called `doccano_labeled.jsonl`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting\n",
    "\n",
    "Split into test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set a seed for reproducability\n",
    "random.seed(42)\n",
    "\n",
    "# Read the annotated data from the Prodigy output\n",
    "with open(\"doccano_labeled.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    annotated_data = f.readlines()\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "random.shuffle(annotated_data)\n",
    "split_idx = int(len(annotated_data) * 0.8)\n",
    "\n",
    "train_data = annotated_data[:split_idx]\n",
    "valid_data = annotated_data[split_idx:]\n",
    "\n",
    "# Write the training and validation data to separate files\n",
    "with open(\"train.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(train_data)\n",
    "\n",
    "with open(\"valid.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.writelines(valid_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function to convert the Prodigy output to spaCy's training data format\n",
    "def convert_to_spacy_format(annotated_data):\n",
    "    spacy_data = []\n",
    "\n",
    "    for line in annotated_data:\n",
    "        data = json.loads(line)\n",
    "        text = data[\"text\"]\n",
    "        spans = {}\n",
    "\n",
    "        for entity in data[\"entities\"]:\n",
    "            start = entity[\"start_offset\"]\n",
    "            end = entity[\"end_offset\"]\n",
    "            label = entity[\"label\"]\n",
    "            spans[label] = []\n",
    "            spans[label].append((start, end))\n",
    "\n",
    "        spacy_data.append((text, {\"spans\": spans}))\n",
    "\n",
    "    return spacy_data\n",
    "\n",
    "# Convert the training and validation data to spaCy format\n",
    "with open(\"train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = f.readlines()\n",
    "\n",
    "with open(\"valid.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    valid_data = f.readlines()\n",
    "\n",
    "train_spacy_data = convert_to_spacy_format(train_data)\n",
    "valid_spacy_data = convert_to_spacy_format(valid_data)\n",
    "\n",
    "# Save the spaCy data to separate files\n",
    "with open(\"train_spacy.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_spacy_data, f)\n",
    "\n",
    "with open(\"valid_spacy.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(valid_spacy_data, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0: {'ner': 0.0}\n",
      "Losses at iteration 1: {'ner': 0.0}\n",
      "Losses at iteration 2: {'ner': 0.0}\n",
      "Losses at iteration 3: {'ner': 0.0}\n",
      "Losses at iteration 4: {'ner': 0.0}\n",
      "Losses at iteration 5: {'ner': 0.0}\n",
      "Losses at iteration 6: {'ner': 0.0}\n",
      "Losses at iteration 7: {'ner': 0.0}\n",
      "Losses at iteration 8: {'ner': 0.0}\n",
      "Losses at iteration 9: {'ner': 0.0}\n",
      "Losses at iteration 10: {'ner': 0.0}\n",
      "Losses at iteration 11: {'ner': 0.0}\n",
      "Losses at iteration 12: {'ner': 0.0}\n",
      "Losses at iteration 13: {'ner': 0.0}\n",
      "Losses at iteration 14: {'ner': 0.0}\n",
      "Losses at iteration 15: {'ner': 0.0}\n",
      "Losses at iteration 16: {'ner': 0.0}\n",
      "Losses at iteration 17: {'ner': 0.0}\n",
      "Losses at iteration 18: {'ner': 0.0}\n",
      "Losses at iteration 19: {'ner': 0.0}\n",
      "Spans in 'Israeli Prime Minister Benjamin Netanyahu visited the White House today. Israel, Palestine, Judaism.'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# read the annotated data\n",
    "with open(\"train_spacy.json\", \"r\") as f:\n",
    "    TRAIN_DATA = json.load(f)\n",
    "\n",
    "# Load the model and set up the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "# Add the labels\n",
    "ner.add_label(\"ISRAEL\")\n",
    "ner.add_label(\"PALESTINE\")\n",
    "ner.add_label(\"JUDAISM\")\n",
    "\n",
    "# define the output directory for the trained model\n",
    "output_dir = Path(\"entity_model\")\n",
    "\n",
    "# Disable other pipelines in spaCy to only train NER\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "    # Set up the optimizer\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for i in range(20):\n",
    "        # Shuffle the training data\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "\n",
    "        # Create batches of training data\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "\n",
    "        # Initialize the losses\n",
    "        losses = {}\n",
    "\n",
    "        # Iterate over the batches\n",
    "        for batch in batches:\n",
    "            # Convert the batch to Examples and update the model\n",
    "            examples = []\n",
    "            for text, annots in batch:\n",
    "                examples.append(Example.from_dict(nlp.make_doc(text), annots))\n",
    "            nlp.update(examples, sgd=optimizer, drop=0.35, losses=losses)\n",
    "\n",
    "        # Print the losses\n",
    "        print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "        # save the trained model to the output directory\n",
    "        nlp.to_disk(output_dir)\n",
    "\n",
    "# test the trained model on some sample text\n",
    "test_text = \"Israeli Prime Minister Benjamin Netanyahu visited the White House today. Israel, Palestine, Judaism.\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Spans in '%s'\" % test_text)\n",
    "for span in doc.spans:\n",
    "    print(span.label_, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Talmud and Jewish scholarship\n",
      "Gold entities:  {'JUDAISM': [[0, 29]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Jerusalem\n",
      "Gold entities:  {'ISRAEL': [[0, 9]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Yitzhak Rabin and the assassination\n",
      "Gold entities:  {'ISRAEL': [[0, 35]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Talmud\n",
      "Gold entities:  {'JUDAISM': [[0, 6]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Hanukkah\n",
      "Gold entities:  {'JUDAISM': [[0, 8]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  An updated version of this World Report chapter is available here >>\n",
      "Gold entities:  {}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Al-Aqsa Mosque\n",
      "Gold entities:  {'PALESTINE': [[0, 14]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Laws in Gaza punish “unnatural intercourse” of a sexual nature, understood to include same-sex relationships, with up to 10 years in prison.\n",
      "Gold entities:  {'PALESTINE': [[8, 13]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Torah\n",
      "Gold entities:  {'JUDAISM': [[0, 5]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Israelis largely failed to hold accountable security forces who used excessive force against Palestinians or settlers who attacked Palestinians and destroyed or damaged their homes and other property.\n",
      "Gold entities:  {'ISRAEL': [[109, 118]], 'PALESTINE': [[131, 144]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Human Rights Watch defends the rights of people in close to 100 countries worldwide, spotlighting abuses and bringing perpetrators to justice\n",
      "Gold entities:  {}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Israeli-Palestinian coexistence initiatives\n",
      "Gold entities:  {'ISRAEL': [[0, 43]], 'PALESTINE': [[0, 43]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Gaza\n",
      "Gold entities:  {'ISRAEL': [[0, 4]], 'PALESTINE': [[0, 4]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Iron Dome\n",
      "Gold entities:  {'ISRAEL': [[0, 9]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Share this via Facebook\n",
      "\n",
      "\n",
      "\n",
      "Share this via Twitter\n",
      "\n",
      "\n",
      "\n",
      "Share this via WhatsApp\n",
      "\n",
      "\n",
      "\n",
      "Share this via Email\n",
      "\n",
      "\n",
      "\n",
      "Other ways to share\n",
      "\n",
      "\n",
      "\n",
      "Share this via LinkedIn\n",
      "\n",
      "\n",
      "\n",
      "Share this via Reddit\n",
      "\n",
      "\n",
      "\n",
      "Share this via Telegram\n",
      "\n",
      "\n",
      "\n",
      "Share this via Printer\n",
      "Gold entities:  {}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Text:  Israel continued to provide security, infrastructure administrative services, housing, education, and medical care for more than 642,867 settlers residing in unlawful settlements in the West Bank, including East Jerusalem.\n",
      "Gold entities:  {'ISRAEL': [[212, 221]], 'PALESTINE': [[207, 221]]}\n",
      "Predicted entities:  []\n",
      "Precision:  0\n",
      "Recall:  0\n",
      "F1-score:  0\n",
      "---------------------------------------\n",
      "Total number of true positives:  0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "\n",
    "# Load the saved NER model\n",
    "nlp = spacy.load('entity_model')\n",
    "\n",
    "# Load the validation data\n",
    "with open('valid_spacy.json', 'r') as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "true_positive_sum = 0\n",
    "\n",
    "# Iterate over the validation data and test the model\n",
    "for data in validation_data:\n",
    "    text = data[0]\n",
    "    gold_spans = data[1]['spans']\n",
    "    doc = nlp(text)\n",
    "    predicted_spans = [(span.start_char, span.end_char, span.label_) for span in doc.spans]\n",
    "    \n",
    "    # Compare the predicted entities to the gold-standard entities\n",
    "    # and print the evaluation metrics\n",
    "    true_positives = set(predicted_spans) & set(gold_spans)\n",
    "    true_positive_sum += len(true_positives)\n",
    "    false_positives = set(predicted_spans) - set(gold_spans)\n",
    "    false_negatives = set(gold_spans) - set(predicted_spans)\n",
    "    \n",
    "    if(len(true_positives) == 0):\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        precision = len(true_positives) / (len(true_positives) + len(false_positives))\n",
    "        recall = len(true_positives) / (len(true_positives) + len(false_negatives))\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print('Text: ', text)\n",
    "    print('Gold entities: ', gold_spans)\n",
    "    print('Predicted entities: ', predicted_spans)\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('F1-score: ', f1_score)\n",
    "    print('---------------------------------------')\n",
    "\n",
    "print(\"Total number of true positives: \",true_positive_sum)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
