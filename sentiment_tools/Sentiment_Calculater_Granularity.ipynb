{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Xo6B9XFLNyi1"
      },
      "outputs": [],
      "source": [
        "SCHOOL = \"USF\"\n",
        "model = \"nltk_sia\" # models can be \"nltk_sia\" or \"bart\"\n",
        "data_path = f\"../bias_processing/data/1/{SCHOOL.lower()}_dataset.neutral.csv\"\n",
        "output_path = f\"../bias_processing/data/2/{model}/{SCHOOL.lower()}_dataset_granularity.neutral.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "5Yhe-9D5Nyi2",
        "outputId": "2c72fd24-7c65-46de-cd3d-313a99446760"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nLoad in a csv from Sentiment_Dataset_Maker and add 4x3x3 columns\\n4 topics (\"Israel\", \"Palestine\", \"India\", \"China\")\\n3 hypotheses for sentiment (Positive, Negative, Neutral)\\n3 levels of granularity\\nCompute sentiment for entire article\\nCompute sentiment for each paragraph in an article, take maximum for Positive and Negative, average for neutral\\nMake \\'average\\' and \\'max\\' options to a function call so we can change if need be\\nCompute sentiment for each sentence in an article, similar to paragraph approach\\nSave a new csv with these added columns\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "Load in a csv from Sentiment_Dataset_Maker and add 4x3x3 columns\n",
        "4 topics (\"Israel\", \"Palestine\", \"India\", \"China\")\n",
        "3 hypotheses for sentiment (Positive, Negative, Neutral)\n",
        "3 levels of granularity\n",
        "Compute sentiment for entire article\n",
        "Compute sentiment for each paragraph in an article, take maximum for Positive and Negative, average for neutral\n",
        "Make 'average' and 'max' options to a function call so we can change if need be\n",
        "Compute sentiment for each sentence in an article, similar to paragraph approach\n",
        "Save a new csv with these added columns\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgIGKNR8Nyi4",
        "outputId": "c3650c1c-8e68-426c-c485-f48489e3b0ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /Users/Dana/Library/Python/3.9/lib/python/site-packages (4.31.0)\n",
            "Requirement already satisfied: nltk in /Users/Dana/Library/Python/3.9/lib/python/site-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.8.8)\n",
            "Requirement already satisfied: requests in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: click in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: fsspec in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/Dana/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.7.22)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /Users/Dana/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/Dana/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcUTfUvpNyi6",
        "outputId": "101a0a16-d53a-4fa8-9a71-b7c780f70b59"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "\n",
        "# Function to return the sentiment of a text\n",
        "def get_sentiment(text, granularity, keyword, model=model, method='avg'):\n",
        "    if model == \"nltk_sia\":\n",
        "        # Instantiate the sentiment analyzer\n",
        "        sia = SentimentIntensityAnalyzer()\n",
        "        # Output is a dict containing {'neg','pos','neu','composition'}. First three are needed for all future functionality\n",
        "        def get_model_scores(text):\n",
        "            scores = sia.polarity_scores(text)\n",
        "            return scores\n",
        "        def get_keys(text):\n",
        "            return sia.polarity_scores(text).keys()\n",
        "\n",
        "    elif model == \"bart\":\n",
        "        bart_analyzer = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "        def get_model_scores(text):\n",
        "            labels = ['positive', 'negative', 'neutral']\n",
        "            scores = bart_analyzer(text, labels)['scores']\n",
        "            scores = {'pos': scores[0], 'neg': scores[1], 'neu': scores[2]}\n",
        "            return scores\n",
        "        def get_keys(text):\n",
        "            return ['pos', 'neg', 'neu']\n",
        "\n",
        "    if granularity == 'article':\n",
        "        scores = get_model_scores(text)\n",
        "        return scores['neg'], scores['pos'], scores['neu']\n",
        "    elif granularity in ['paragraph', 'sentence']:\n",
        "        if granularity == 'paragraph':\n",
        "            # Calculate the polarity scores for each paragraph and store them in a list\n",
        "            # TODO: Revise and check paragraph splitting, may have issues with article splitting\n",
        "            listed_scores = [get_model_scores(paragraph) for paragraph in text.split('\\n') if paragraph]\n",
        "        elif granularity == 'sentence':\n",
        "            listed_scores = [get_model_scores(sentence) for sentence in nltk.sent_tokenize(text)]\n",
        "\n",
        "        # Transpose the list of dictionaries to separate the values for each key\n",
        "        transposed_scores = list(zip(*[d.values() for d in listed_scores]))\n",
        "\n",
        "        # Find the maximum value for each key using the max function\n",
        "        ranked_scores = [max(scores) for scores in transposed_scores]\n",
        "\n",
        "        # Create a dictionary with the corresponding keys and maximum values\n",
        "        result_dict = dict(zip(get_keys(text), ranked_scores))\n",
        "\n",
        "        return result_dict['neg'], result_dict['pos'], result_dict['neu']\n",
        "\n",
        "# # Load the csv\n",
        "# df = pd.read_csv(data_path)\n",
        "\n",
        "# # Process sentiment analysis\n",
        "# for granularity in ['article', 'paragraph', 'sentence']:\n",
        "#     print(granularity)\n",
        "#     df[f'{granularity}_neg'], df[f'{granularity}_pos'], df[f'{granularity}_neu'] = zip(\n",
        "#         *df.apply(lambda row: get_sentiment(row['article'], granularity, row['keyword'], model=model), axis=1)\n",
        "#     )\n",
        "\n",
        "# # Save the output DataFrame into a new CSV file\n",
        "# df.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article: \n",
            "... | Keyword: study | Granularity: article | Neg: 0.0 | Pos: 0.0 | Neu: 0.0\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'neg'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msentiment_calculater\u001b[39;00m \u001b[39mimport\u001b[39;00m build_csv\n\u001b[0;32m----> 3\u001b[0m build_csv(data_path, output_path, get_sentiment)\n",
            "File \u001b[0;32m~/Desktop/Olami/sentiment_tools/sentiment_calculater.py:44\u001b[0m, in \u001b[0;36mbuild_csv\u001b[0;34m(data_path, output_path, get_sentiment)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39m# Perform sentiment analysis for each granularity\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m granularity \u001b[39min\u001b[39;00m granularities:\n\u001b[0;32m---> 44\u001b[0m     neg, pos, neu \u001b[39m=\u001b[39m get_sentiment(article, granularity, keyword)\n\u001b[1;32m     46\u001b[0m     \u001b[39m# Prepare the row to write to the CSV file\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     output_row\u001b[39m.\u001b[39mextend([neg, pos, neu])\n",
            "Cell \u001b[0;32mIn[17], line 48\u001b[0m, in \u001b[0;36mget_sentiment\u001b[0;34m(text, granularity, keyword, model, method)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39m# Create a dictionary with the corresponding keys and maximum values\u001b[39;00m\n\u001b[1;32m     46\u001b[0m result_dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(get_keys(text), ranked_scores))\n\u001b[0;32m---> 48\u001b[0m \u001b[39mreturn\u001b[39;00m result_dict[\u001b[39m'\u001b[39;49m\u001b[39mneg\u001b[39;49m\u001b[39m'\u001b[39;49m], result_dict[\u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m], result_dict[\u001b[39m'\u001b[39m\u001b[39mneu\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\u001b[0;31mKeyError\u001b[0m: 'neg'"
          ]
        }
      ],
      "source": [
        "from sentiment_calculater import build_csv\n",
        "\n",
        "build_csv(data_path, output_path, get_sentiment)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
