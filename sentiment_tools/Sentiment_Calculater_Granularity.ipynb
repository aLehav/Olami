{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xo6B9XFLNyi1"
      },
      "outputs": [],
      "source": [
        "# Define SCHOOL, data_path, and output_path\n",
        "SCHOOL = \"LIU\"\n",
        "model = \"nltk_sia\" # models can be \"nltk_sia\" or \"bart\"\n",
        "data_path = f\"{SCHOOL.lower()}_dataset.csv\"\n",
        "output_path = f\"{SCHOOL.lower()}_dataset_granularity.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5Yhe-9D5Nyi2",
        "outputId": "2c72fd24-7c65-46de-cd3d-313a99446760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLoad in a csv from Sentiment_Dataset_Maker and add 4x3x3 columns\\n4 topics (\"Israel\", \"Palestine\", \"India\", \"China\")\\n3 hypotheses for sentiment (Positive, Negative, Neutral)\\n3 levels of granularity\\nCompute sentiment for entire article\\nCompute sentiment for each paragraph in an article, take maximum for Positive and Negative, average for neutral\\nMake \\'average\\' and \\'max\\' options to a function call so we can change if need be\\nCompute sentiment for each sentence in an article, similar to paragraph approach\\nSave a new csv with these added columns\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "Load in a csv from Sentiment_Dataset_Maker and add 4x3x3 columns\n",
        "4 topics (\"Israel\", \"Palestine\", \"India\", \"China\")\n",
        "3 hypotheses for sentiment (Positive, Negative, Neutral)\n",
        "3 levels of granularity\n",
        "Compute sentiment for entire article\n",
        "Compute sentiment for each paragraph in an article, take maximum for Positive and Negative, average for neutral\n",
        "Make 'average' and 'max' options to a function call so we can change if need be\n",
        "Compute sentiment for each sentence in an article, similar to paragraph approach\n",
        "Save a new csv with these added columns\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CgIGKNR8Nyi4",
        "outputId": "c3650c1c-8e68-426c-c485-f48489e3b0ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk pandas\n",
        "\n",
        "%pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pcUTfUvpNyi6",
        "outputId": "101a0a16-d53a-4fa8-9a71-b7c780f70b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "article\n",
            "paragraph\n",
            "sentence\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Instantiate the sentiment analyzer\n",
        "\n",
        "# Function to return the sentiment of a text\n",
        "def get_sentiment(text, granularity, keyword, model=model, method='avg'):\n",
        "    if model == \"nltk_sia\":\n",
        "        sia = SentimentIntensityAnalyzer()\n",
        "        # Output is a dict containing {'neg','pos','neu','composition'}. First three are needed for all future functionality\n",
        "        def get_model_scores(text):\n",
        "            scores = sia.polarity_scores(text)\n",
        "            return scores\n",
        "        def get_keys(text):\n",
        "            return sia.polarity_scores(text).keys()\n",
        "\n",
        "    elif model == \"bart\":\n",
        "        bart_analyzer = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "        def get_model_scores(text):\n",
        "            labels = ['positive', 'negative', 'neutral']\n",
        "            scores = bart_analyzer(text, labels)['scores']\n",
        "            scores = {'pos': scores[0], 'neg': scores[1], 'neu': scores[2]}\n",
        "            return scores\n",
        "        def get_keys(text):\n",
        "            return ['pos', 'neg', 'neu']\n",
        "\n",
        "    if granularity == 'article':\n",
        "        scores = get_model_scores(text)\n",
        "        return scores['neg'], scores['pos'], scores['neu']\n",
        "    elif granularity in ['paragraph', 'sentence']:\n",
        "        if granularity == 'paragraph':\n",
        "            # Calculate the polarity scores for each paragraph and store them in a list\n",
        "            # TODO: Revise and check paragraph splitting, may have issues with article splitting\n",
        "            listed_scores = [get_model_scores(paragraph) for paragraph in text.split('\\n') if paragraph]\n",
        "        elif granularity == 'sentence':\n",
        "            listed_scores = [get_model_scores(sentence) for sentence in nltk.sent_tokenize(text)]\n",
        "\n",
        "        # Transpose the list of dictionaries to separate the values for each key\n",
        "        transposed_scores = list(zip(*[d.values() for d in listed_scores]))\n",
        "\n",
        "        # Find the maximum value for each key using the max function\n",
        "        ranked_scores = [max(scores) for scores in transposed_scores]\n",
        "\n",
        "        # Create a dictionary with the corresponding keys and maximum values\n",
        "        result_dict = dict(zip(get_keys(text), ranked_scores))\n",
        "\n",
        "        return result_dict['neg'], result_dict['pos'], result_dict['neu']\n",
        "\n",
        "# Load the csv\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Process sentiment analysis\n",
        "for granularity in ['article', 'paragraph', 'sentence']:\n",
        "    print(granularity)\n",
        "    df[f'{granularity}_neg'], df[f'{granularity}_pos'], df[f'{granularity}_neu'] = zip(\n",
        "        *df.apply(lambda row: get_sentiment(row['article'], granularity, row['keyword'], model=model), axis=1)\n",
        "    )\n",
        "\n",
        "# Save the output DataFrame into a new CSV file\n",
        "df.to_csv(output_path, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}