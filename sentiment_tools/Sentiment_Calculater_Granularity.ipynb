{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xo6B9XFLNyi1"
      },
      "outputs": [],
      "source": [
        "SCHOOL = \"LIU\"\n",
        "model = \"nltk_sia\" # models can be \"nltk_sia\" or \"bart\"\n",
        "data_path = f\"../bias_processing/data/1/{SCHOOL.lower()}_dataset.csv\"\n",
        "output_path = f\"../bias_processing/data/2/{model}/{SCHOOL.lower()}_dataset_granularity.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "5Yhe-9D5Nyi2",
        "outputId": "2c72fd24-7c65-46de-cd3d-313a99446760"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Load in a csv from Sentiment_Dataset_Maker and add 4x3x3 columns\n",
        "4 topics (\"Israel\", \"Palestine\", \"India\", \"China\")\n",
        "3 hypotheses for sentiment (Positive, Negative, Neutral)\n",
        "3 levels of granularity\n",
        "Compute sentiment for entire article\n",
        "Compute sentiment for each paragraph in an article, take maximum for Positive and Negative, average for neutral\n",
        "Make 'average' and 'max' options to a function call so we can change if need be\n",
        "Compute sentiment for each sentence in an article, similar to paragraph approach\n",
        "Save a new csv with these added columns\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgIGKNR8Nyi4",
        "outputId": "c3650c1c-8e68-426c-c485-f48489e3b0ea"
      },
      "outputs": [],
      "source": [
        "%pip install transformers nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\adaml\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\adaml\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcUTfUvpNyi6",
        "outputId": "101a0a16-d53a-4fa8-9a71-b7c780f70b59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\adaml\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "\n",
        "# Function to return the sentiment of a text\n",
        "def get_sentiment(text, granularity, keyword, model=model, method='avg'):\n",
        "    if model == \"nltk_sia\":\n",
        "        # Instantiate the sentiment analyzer\n",
        "        sia = SentimentIntensityAnalyzer()\n",
        "        # Output is a dict containing {'neg','pos','neu','composition'}. First three are needed for all future functionality\n",
        "        def get_model_scores(text):\n",
        "            scores = sia.polarity_scores(text)\n",
        "            return scores\n",
        "        def get_keys(text):\n",
        "            return sia.polarity_scores(text).keys()\n",
        "\n",
        "    elif model == \"bart\":\n",
        "        bart_analyzer = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "        def get_model_scores(text):\n",
        "            labels = ['positive', 'negative', 'neutral']\n",
        "            scores = bart_analyzer(text, labels)['scores']\n",
        "            scores = {'pos': scores[0], 'neg': scores[1], 'neu': scores[2]}\n",
        "            return scores\n",
        "        def get_keys(text):\n",
        "            return ['pos', 'neg', 'neu']\n",
        "\n",
        "    if granularity == 'article':\n",
        "        scores = get_model_scores(text)\n",
        "        return scores['neg'], scores['pos'], scores['neu']\n",
        "    elif granularity in ['paragraph', 'sentence']:\n",
        "        if granularity == 'paragraph':\n",
        "            # Calculate the polarity scores for each paragraph and store them in a list\n",
        "            # TODO: Revise and check paragraph splitting, may have issues with article splitting\n",
        "            listed_scores = [get_model_scores(paragraph) for paragraph in text.split('\\n') if paragraph]\n",
        "        elif granularity == 'sentence':\n",
        "            listed_scores = [get_model_scores(sentence) for sentence in nltk.sent_tokenize(text)]\n",
        "\n",
        "        # Transpose the list of dictionaries to separate the values for each key\n",
        "        transposed_scores = list(zip(*[d.values() for d in listed_scores]))\n",
        "\n",
        "        # Find the maximum value for each key using the max function\n",
        "        ranked_scores = [max(scores) for scores in transposed_scores]\n",
        "\n",
        "        # Create a dictionary with the corresponding keys and maximum values\n",
        "        result_dict = dict(zip(get_keys(text), ranked_scores))\n",
        "\n",
        "        return result_dict['neg'], result_dict['pos'], result_dict['neu']\n",
        "\n",
        "# # Load the csv\n",
        "# df = pd.read_csv(data_path)\n",
        "\n",
        "# # Process sentiment analysis\n",
        "# for granularity in ['article', 'paragraph', 'sentence']:\n",
        "#     print(granularity)\n",
        "#     df[f'{granularity}_neg'], df[f'{granularity}_pos'], df[f'{granularity}_neu'] = zip(\n",
        "#         *df.apply(lambda row: get_sentiment(row['article'], granularity, row['keyword'], model=model), axis=1)\n",
        "#     )\n",
        "\n",
        "# # Save the output DataFrame into a new CSV file\n",
        "# df.to_csv(output_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentiment_calculater import build_csv\n",
        "\n",
        "build_csv(data_path, output_path, get_sentiment)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
