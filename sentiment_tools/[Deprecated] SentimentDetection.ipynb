{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positive Count Grabber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to set #####################################\n",
    "SCHOOL_NAME = \"York\"\n",
    "PIPELINE=[]\n",
    "STRINGS = [\"China\", \"India\", \"Israel\", \"Palestine\"]\n",
    "###################################################\n",
    "\n",
    "ARTICLE_JSON_PATH = f'{SCHOOL_NAME.lower()}_article_pages.json'\n",
    "\n",
    "csv_path = 'grouped_data/csv/'\n",
    "def get_csv_file(string):\n",
    "    return f'{csv_path}{SCHOOL_NAME}_{string}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import json\n",
    "from helpers.queries import count_query\n",
    "from helpers.processors import csv_to_positive_articles, schools_pipeline_query_to_csv\n",
    "\n",
    "def runner(school_name,\n",
    "           pipeline,\n",
    "           string,\n",
    "           csv_file,\n",
    "           article_json_path):\n",
    "    def generate_positive_csv():\n",
    "        schools_pipeline_query_to_csv(school_names=[school_name], \n",
    "                                    pipeline=pipeline, \n",
    "                                    query=count_query(string), \n",
    "                                    csv_file=csv_file)\n",
    "        df = csv_to_positive_articles(csv_file)\n",
    "        return df\n",
    "\n",
    "    def make_links_field(df):\n",
    "        with open(article_json_path, 'r') as f:\n",
    "            ucf_article_pages = json.load(f)\n",
    "\n",
    "        df['links'] = df.apply(lambda row: ucf_article_pages[row['date'].replace('-','_')], axis=1)\n",
    "\n",
    "    def make_text_field(df):\n",
    "        def split_text(school_name):\n",
    "            if school_name == \"UCF\":\n",
    "                def split_text_ucf(row):\n",
    "                    with open(row['txt_file'], 'r', encoding='utf-8') as f:\n",
    "                        text = f.read()\n",
    "                    return re.split(r'\\n{3,}', text)[:-1]\n",
    "                return split_text_ucf\n",
    "            elif school_name == 'York':\n",
    "                def split_text_york(row):\n",
    "                    with open(row['txt_file'], 'r', encoding='utf-8') as f:\n",
    "                        text = f.read()\n",
    "                    texts = re.split(r'\\n{3,}', text)\n",
    "                    if len(texts) > 1:\n",
    "                        texts = texts[:-1]\n",
    "                    return texts\n",
    "                return split_text_york\n",
    "            else:\n",
    "                raise ValueError(f\"School {school_name} not yet supported.\")\n",
    "\n",
    "        df['text'] = df.apply(split_text(school_name=school_name), axis=1)\n",
    "\n",
    "    def text_link_checker(df):\n",
    "        for index, row in df.iterrows():\n",
    "            if len(row['text']) != len(row['links']):\n",
    "                print(f\"Date {row['date']}: Length of 'text' is {len(row['text'])}, while length of 'links' is {len(row['links'])}\")\n",
    "\n",
    "    def make_matching_text_field(df):\n",
    "        def matching_text(row):\n",
    "            indices = [count_query(string)(text) for text in row['text']]\n",
    "            new_text = [value for value, index in zip(row['text'], indices) if index != 0]\n",
    "            return new_text\n",
    "        df['matching_text'] = df.apply(matching_text, axis=1)\n",
    "\n",
    "    def make_matching_links_field(df):\n",
    "        def matching_links(row):\n",
    "            indices = [count_query(string)(text) for text in row['text']]\n",
    "            new_text = [value for value, index in zip(row['links'], indices) if index != 0]\n",
    "            return new_text\n",
    "        df['matching_links'] = df.apply(matching_links, axis=1)\n",
    "\n",
    "    def save_df(df):\n",
    "        df.to_csv(f'{school_name}_{string}.csv')\n",
    "        \n",
    "    df = generate_positive_csv()\n",
    "    make_links_field(df)\n",
    "    make_text_field(df)\n",
    "    text_link_checker(df)\n",
    "    make_matching_links_field(df)\n",
    "    make_matching_text_field(df)\n",
    "    save_df(df)\n",
    "    print(f'Finished working with {SCHOOL_NAME} for {string}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grouped_data/csv/York_China.csv already exists.\n",
      "Date 2011-04-18: Length of 'text' is 2, while length of 'links' is 1\n",
      "Date 2011-09-28: Length of 'text' is 11, while length of 'links' is 9\n",
      "Date 2020-10-29: Length of 'text' is 1, while length of 'links' is 2\n",
      "Finished working with York for China.\n",
      "grouped_data/csv/York_India.csv already exists.\n",
      "Date 2014-08-27: Length of 'text' is 24, while length of 'links' is 10\n",
      "Date 2020-10-30: Length of 'text' is 1, while length of 'links' is 2\n",
      "Date 2021-01-29: Length of 'text' is 1, while length of 'links' is 2\n",
      "Date 2021-02-25: Length of 'text' is 1, while length of 'links' is 2\n",
      "Date 2023-01-30: Length of 'text' is 3, while length of 'links' is 4\n",
      "Finished working with York for India.\n",
      "grouped_data/csv/York_Israel.csv already exists.\n",
      "Date 2010-10-27: Length of 'text' is 13, while length of 'links' is 10\n",
      "Date 2011-01-26: Length of 'text' is 10, while length of 'links' is 9\n",
      "Date 2011-02-17: Length of 'text' is 6, while length of 'links' is 4\n",
      "Date 2013-12-09: Length of 'text' is 4, while length of 'links' is 3\n",
      "Finished working with York for Israel.\n",
      "grouped_data/csv/York_Palestine.csv already exists.\n",
      "Date 2011-09-28: Length of 'text' is 11, while length of 'links' is 9\n",
      "Finished working with York for Palestine.\n"
     ]
    }
   ],
   "source": [
    "for STRING in STRINGS:\n",
    "       runner(school_name=SCHOOL_NAME,\n",
    "              pipeline=PIPELINE,\n",
    "              string=STRING,\n",
    "              csv_file=get_csv_file(STRING),\n",
    "              article_json_path=ARTICLE_JSON_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretical text processing, to be used in the case of pdf text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adaml\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# # Load in the data\n",
    "# import nltk\n",
    "# from helpers.processors import positive_articles_to_sentences\n",
    "# from helpers.processors import preprocess_text\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# txt_list = positive_articles_to_sentences(df=df, string=string)\n",
    "# proc_txt_list = [preprocess_text(txt) for txt in txt_list]\n",
    "# process_checker = pd.DataFrame([txt_list, proc_txt_list]).transpose()\n",
    "# process_checker.columns = [\"Raw\", \"Processed\"]\n",
    "# process_checker.to_csv(\"example_processed_text.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching all associated text:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
