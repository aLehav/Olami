{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "from tmtoolkit import corpus as c\n",
    "from tmtoolkit.topicmod import tm_gensim\n",
    "from tmtoolkit.utils import pickle_data, enable_logging\n",
    "from tmtoolkit.topicmod.evaluate import results_by_parameter\n",
    "from tmtoolkit.topicmod.visualize import plot_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "loaded 1000 documents\n"
     ]
    }
   ],
   "source": [
    "print('loading data...')\n",
    "bt18 = pd.read_pickle('data/bt18_sample_1000.pickle')\n",
    "print('loaded %d documents' % len(bt18))\n",
    "doc_labels = ['%s_%s' % info for info in zip(bt18.sitzung, bt18.sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and tokenizing documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 22:17:32,060:INFO:tmtoolkit:creating Corpus instance with 1000 documents\n",
      "2023-03-26 22:17:32,061:INFO:tmtoolkit:using parallel processing with 12 workers\n",
      "2023-03-26 22:17:32,062:INFO:tmtoolkit:running NLP pipeline on 1000 documents\n",
      "2023-03-26 22:18:53,309:INFO:tmtoolkit:generating document texts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 1000 documents in German\n",
      "> 101_6369 (854 tokens): Herr Präsident ! Liebe Kolleginnen und Kollegen ! ...\n",
      "> 101_6387 (2230 tokens): Frau Präsidentin ! Liebe Kolleginnen und Kollegen ...\n",
      "> 100_6325 (584 tokens): Sehr geehrte Frau Präsidentin ! Liebe Kolleginnen ...\n",
      "> 100_6278 (635 tokens): Sehr geehrte Frau Präsidentin ! Meine sehr geehrte...\n",
      "> 102_6453 (881 tokens): Vielen Dank . - Frau Präsidentin ! Meine Damen und...\n",
      "> 101_6392 (1973 tokens): Sehr verehrte Frau Präsidentin ! Sehr verehrte Kol...\n",
      "> 102_6455 (906 tokens): Frau Präsidentin ! Liebe Kolleginnen und Kollegen ...\n",
      "> 101_6376 (1225 tokens): Herr Präsident ! Meine Damen und Herren ! Herr Zim...\n",
      "> 102_6429 (72 tokens): Herr Bundesminister Gabriel , Sie haben vorhin in ...\n",
      "> 100_6319 (656 tokens): Sehr geehrter Herr Präsident ! Liebe Gäste auf den...\n",
      "(and 990 more documents)\n",
      "total number of tokens: 925947 / vocabulary size: 48728\n"
     ]
    }
   ],
   "source": [
    "print('loading and tokenizing documents')\n",
    "# minimal pipeline\n",
    "bt18corp = c.Corpus(dict(zip(doc_labels, bt18.text)), language='de', load_features=[], max_workers=1.0)\n",
    "del bt18\n",
    "c.print_summary(bt18corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 22:19:02,995:INFO:tmtoolkit:replacing 43993 token hashes\n",
      "2023-03-26 22:19:04,338:INFO:tmtoolkit:filtered tokens by mask: num. tokens was 925947 and is now 321112\n",
      "2023-03-26 22:19:04,825:INFO:tmtoolkit:generating document texts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus with 1000 documents in German\n",
      "> 101_6369 (295 tokens): herr prasident lieb kolleginn kolleg beitrag frag ...\n",
      "> 101_6387 (776 tokens): frau prasidentin lieb kolleginn kolleg dam herr no...\n",
      "> 100_6325 (225 tokens): geehrt frau prasidentin lieb kolleginn kolleg deut...\n",
      "> 100_6278 (229 tokens): geehrt frau prasidentin geehrt dam herr energiekon...\n",
      "> 102_6453 (339 tokens): frau prasidentin dam herr herr kolleg strobel fakt...\n",
      "> 101_6392 (676 tokens): verehrt frau prasidentin verehrt kolleginn geehrt ...\n",
      "> 102_6455 (326 tokens): frau prasidentin lieb kolleginn kolleg froh heutig...\n",
      "> 101_6376 (421 tokens): herr prasident dam herr herr zimm wort begriff kre...\n",
      "> 102_6429 (28 tokens): herr bundesminist gabriel vorhin bericht ausgefuhr...\n",
      "> 100_6319 (261 tokens): geehrt herr prasident lieb gast tribun wert kolleg...\n",
      "(and 990 more documents)\n",
      "total number of tokens: 321112 / vocabulary size: 33418\n"
     ]
    }
   ],
   "source": [
    "print('preprocessing data...')\n",
    "\n",
    "c.stem(bt18corp)\n",
    "c.filter_clean_tokens(bt18corp)\n",
    "\n",
    "c.print_summary(bt18corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating gensim corpus...\n"
     ]
    }
   ],
   "source": [
    "print('creating gensim corpus...')\n",
    "\n",
    "texts = list(c.doc_tokens(bt18corp).values())\n",
    "gnsm_dict = gensim.corpora.Dictionary.from_documents(texts)\n",
    "gnsm_corpus = [gnsm_dict.doc2bow(text) for text in texts]\n",
    "\n",
    "del bt18corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 22:19:17,852:INFO:tmtoolkit:init with 12 workers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating 16 topic models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-26 22:20:38,280:INFO:tmtoolkit:multiproc models: starting with 16 parameter sets on 1 datasets (= 16 tasks) and 12 processes\n"
     ]
    }
   ],
   "source": [
    "# evaluate topic models with different parameters\n",
    "const_params = dict(update_every=0, passes=10)\n",
    "ks = list(range(10, 140, 10)) + list(range(140, 200, 20))\n",
    "varying_params = [dict(num_topics=k, alpha=1.0 / k) for k in ks]\n",
    "\n",
    "print(f'evaluating {len(varying_params)} topic models')\n",
    "eval_results = tm_gensim.evaluate_topic_models((gnsm_dict, gnsm_corpus), varying_params, const_params,\n",
    "                                               coherence_gensim_texts=texts)   # necessary for coherence C_V metric\n",
    "\n",
    "# save the results as pickle\n",
    "print('saving results')\n",
    "pickle_data(eval_results, 'data/gensim_evaluation_results.pickle')\n",
    "\n",
    "# plot the results\n",
    "print('plotting evaluation results')\n",
    "plt.style.use('ggplot')\n",
    "results_by_n_topics = results_by_parameter(eval_results, 'num_topics')\n",
    "plot_eval_results(results_by_n_topics, xaxislabel='num. topics k',\n",
    "                  title='Evaluation results', figsize=(8, 6))\n",
    "plt.savefig('data/gensim_evaluation_plot.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
