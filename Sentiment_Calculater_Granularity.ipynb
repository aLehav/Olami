{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SCHOOL, data_path, and output_path\n",
    "SCHOOL = \"McGill\"\n",
    "data_path = f\"bias_processing/data/1/{SCHOOL.lower()}_dataset.csv\"\n",
    "output_path = f\"bias_processing/data/2/{SCHOOL.lower()}_dataset_granularity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLoad in a csv from Sentiment_Dataset_Maker and add 4x3x3 columns\\n4 topics (\"Israel\", \"Palestine\", \"India\", \"China\")\\n3 hypotheses for sentiment (Positive, Negative, Neutral)\\n3 levels of granularity\\nCompute sentiment for entire article\\nSummarizes each paragraph using an ML summarizing model, and join those summaries to one body of text. Compute sentiment for this new article version.\\nSummarize the entire article in one go using the same ML model. Compute sentiment for this new article version\\nSave a new csv with these added columns\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load in a csv from Sentiment_Dataset_Maker and add 4x3x3 columns\n",
    "4 topics (\"Israel\", \"Palestine\", \"India\", \"China\")\n",
    "3 hypotheses for sentiment (Positive, Negative, Neutral)\n",
    "3 levels of granularity\n",
    "Compute sentiment for entire article\n",
    "Summarizes each paragraph using an ML summarizing model, and join those summaries to one body of text. Compute sentiment for this new article version.\n",
    "Summarize the entire article in one go using the same ML model. Compute sentiment for this new article version\n",
    "Save a new csv with these added columns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\adaml\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\adaml\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adaml\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "os.environ['NLTK_DATA'] = r\"\\Users\\stacy\\Desktop\\Olami Project\\Olami\"\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article\n",
      "paragraph\n",
      "sentence\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from statistics import mean\n",
    "\n",
    "# Instantiate the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to return the sentiment of a text\n",
    "def get_sentiment(text, granularity, keyword, model=\"nltk_sia\", method='avg'):\n",
    "    if model == \"nltk_sia\":\n",
    "        # Output is a dict containing {'neg','pos','neu','composition'}. First three are needed for all future functionality\n",
    "        def get_model_scores(text):\n",
    "            scores = sia.polarity_scores(text)\n",
    "            return scores\n",
    "    if granularity == 'article':\n",
    "        scores = get_model_scores(text)\n",
    "        return scores['neg'], scores['pos'], scores['neu']\n",
    "    elif granularity in ['paragraph','sentence']:\n",
    "        if granularity == 'paragraph':\n",
    "            # Calculate the polarity scores for each paragraph and store them in a list\n",
    "            # TODO: Revise and check paragraph splitting, may have issues with article splitting\n",
    "            listed_scores = [get_model_scores(paragraph) for paragraph in text.split('\\n') if paragraph]\n",
    "        elif granularity == 'sentence':\n",
    "            listed_scores = [get_model_scores(sentence) for sentence in nltk.sent_tokenize(text)]\n",
    "\n",
    "        # Transpose the list of dictionaries to separate the values for each key\n",
    "        transposed_scores = list(zip(*[d.values() for d in listed_scores]))\n",
    "\n",
    "        # Find the maximum value for each key using the max function\n",
    "        if method == 'max':\n",
    "            ranked_scores = [max(scores) for scores in transposed_scores]\n",
    "        elif method == 'avg':\n",
    "            ranked_scores = [sum(scores) / len(scores) for scores in transposed_scores]\n",
    "\n",
    "        # Create a dictionary with the corresponding keys and maximum values\n",
    "        result_dict = dict(zip(sia.polarity_scores(text).keys(), ranked_scores))\n",
    "\n",
    "        return result_dict['neg'], result_dict['pos'], result_dict['neu']\n",
    "\n",
    "# Load the csv\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Process sentiment analysis\n",
    "for granularity in ['article','paragraph','sentence']:\n",
    "    print(granularity)\n",
    "    df[f'{granularity}_neg'], df[f'{granularity}_pos'], df[f'{granularity}_neu'] = zip(*df.apply(lambda row: get_sentiment(row['article'], granularity, row['keyword']), axis=1))\n",
    "        \n",
    "\n",
    "# Save the output DataFrame into a new CSV file\n",
    "df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
